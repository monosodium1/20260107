import sys
import os
import json
import threading
import queue
import time
from datetime import datetime

baidusearch_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../dist/baidusearch'))
if baidusearch_path not in sys.path:
    sys.path.insert(0, baidusearch_path)

try:
    from baidu_search import BaiduSearchCrawler
except ImportError:
    BaiduSearchCrawler = None

from app.models import CrawlerSource, CollectionData
from app.services.universal_crawler import create_crawler
from app import db
from app import create_app


class CollectionService:
    """é‡‡é›†ç®¡ç†æœåŠ¡ç±»"""

    def __init__(self):
        self.active_collections = {}
        self.data_queues = {}
        self.crawler_pool = {}

    def get_available_sources(self):
        """
        è·å–å¯ç”¨çš„çˆ¬è™«æºåˆ—è¡¨

        Returns:
            æºåˆ—è¡¨
        """
        sources = CrawlerSource.query.filter_by(status='active').all()
        return [s.to_dict() for s in sources]

    def start_collection(self, keyword, source_ids, page=1, pages=1, limit=10):
        """
        å¼€å§‹é‡‡é›†ä»»åŠ¡

        Args:
            keyword: æœç´¢å…³é”®è¯
            source_ids: çˆ¬è™«æºIDåˆ—è¡¨
            page: èµ·å§‹é¡µ
            pages: æ€»é¡µæ•°
            limit: æ¯é¡µæ•°é‡

        Returns:
            é‡‡é›†ä»»åŠ¡ID
        """
        collection_id = f"collection_{int(time.time() * 1000)}"
        
        self.data_queues[collection_id] = queue.Queue()
        
        thread = threading.Thread(
            target=self._execute_collection,
            args=(collection_id, keyword, source_ids, page, pages, limit)
        )
        thread.daemon = True
        thread.start()
        
        self.active_collections[collection_id] = {
            'thread': thread,
            'keyword': keyword,
            'source_ids': source_ids,
            'started_at': datetime.utcnow(),
            'status': 'running'
        }
        
        return collection_id

    def _execute_collection(self, collection_id, keyword, source_ids, page, pages, limit):
        """
        æ‰§è¡Œé‡‡é›†ä»»åŠ¡

        Args:
            collection_id: é‡‡é›†ä»»åŠ¡ID
            keyword: æœç´¢å…³é”®è¯
            source_ids: çˆ¬è™«æºIDåˆ—è¡¨
            page: èµ·å§‹é¡µ
            pages: æ€»é¡µæ•°
            limit: æ¯é¡µæ•°é‡
        """
        app = create_app()
        with app.app_context():
            try:
                for source_id in source_ids:
                    source = CrawlerSource.query.get(source_id)
                    if not source or source.status != 'active':
                        continue

                    crawler = None
                    try:
                        # å°è¯•ä»æ± ä¸­è·å–å·²å­˜åœ¨çš„çˆ¬è™«å®ä¾‹
                        if source.id in self.crawler_pool:
                            crawler = self.crawler_pool[source.id]
                            print(f"âœ… ä»æ± ä¸­è·å–çˆ¬è™«: {source.name}")
                        else:
                            source_config = {
                                'name': source.name,
                                'source_type': source.source_type,
                                'url': source.url,
                                'method': source.method,
                                'headers': source.headers,
                                'body_template': source.body_template,
                                'data_selector': source.data_selector,
                                'title_selector': source.title_selector,
                                'url_selector': source.url_selector,
                                'summary_selector': source.summary_selector,
                                'image_selector': source.image_selector,
                                'config': source.config
                            }

                            crawler = create_crawler(source_config)
                            self.crawler_pool[source.id] = crawler
                            print(f"âœ… åˆ›å»ºæ–°çˆ¬è™«: {source.name} (ç±»å‹: {type(crawler).__name__})")

                        for current_page in range(page, page + pages):
                            print(f"ğŸ” å¼€å§‹çˆ¬å–ç¬¬ {current_page} é¡µ...")
                            page_results = crawler.crawl(keyword, current_page, limit)
                            print(f"ğŸ“Š ç¬¬ {current_page} é¡µè·å–åˆ° {len(page_results)} æ¡ç»“æœ")
                            
                            for result in page_results:
                                data_item = {
                                    'collection_id': collection_id,
                                    'title': result.get('title', ''),
                                    'url': result.get('url', ''),
                                    'summary': result.get('summary', ''),
                                    'source': result.get('source', ''),
                                    'image_url': self._extract_image(result),
                                    'keyword': keyword,
                                    'source_type': source.source_type,
                                    'source_name': source.name,
                                    'collected_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                                    'raw_data': json.dumps(result, ensure_ascii=False)
                                }
                                
                                self.data_queues[collection_id].put(data_item)
                                print(f"âœ… æ¨é€æ•°æ®åˆ°é˜Ÿåˆ—: {data_item['title']}")

                    except Exception as e:
                        error_data = {
                            'collection_id': collection_id,
                            'error': str(e),
                            'source_name': source.name if source else 'Unknown'
                        }
                        self.data_queues[collection_id].put(error_data)
                    
                self.data_queues[collection_id].put({'type': 'completed', 'collection_id': collection_id})

            except Exception as e:
                self.data_queues[collection_id].put({
                    'type': 'error',
                    'collection_id': collection_id,
                    'error': str(e)
                })

            finally:
                if collection_id in self.active_collections:
                    self.active_collections[collection_id]['status'] = 'completed'

    def _extract_image(self, result):
        """
        ä»ç»“æœä¸­æå–å›¾ç‰‡URL

        Args:
            result: æœç´¢ç»“æœ

        Returns:
            å›¾ç‰‡URLæˆ–é»˜è®¤å›¾ç‰‡
        """
        # é¦–å…ˆæ£€æŸ¥çˆ¬è™«æ˜¯å¦ç›´æ¥è¿”å›äº†å›¾ç‰‡URL
        image_url = result.get('image', '')
        if image_url:
            return image_url
        
        # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»æ‘˜è¦ä¸­æå–å›¾ç‰‡URL
        summary = result.get('summary', '')
        if 'jpg' in summary or 'png' in summary or 'jpeg' in summary:
            import re
            img_urls = re.findall(r'https?://[^\s]+\.(?:jpg|png|jpeg|gif|webp)', summary)
            if img_urls:
                return img_urls[0]
        
        # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©å‰ç«¯æ˜¾ç¤ºé»˜è®¤å›¾æ ‡
        return ''

    def get_collection_data(self, collection_id):
        """
        è·å–é‡‡é›†æ•°æ®ï¼ˆç”¨äºSSEæ¨é€ï¼‰

        Args:
            collection_id: é‡‡é›†ä»»åŠ¡ID

        Yields:
            æ•°æ®é¡¹
        """
        if collection_id not in self.data_queues:
            yield f"data: {json.dumps({'error': 'Collection not found'})}\n\n"
            return

        data_queue = self.data_queues[collection_id]
        
        while True:
            try:
                data = data_queue.get(timeout=1)
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
                if data.get('type') == 'completed' or data.get('type') == 'error':
                    break
                    
            except queue.Empty:
                continue

    def save_collection_data(self, data_items):
        """
        ä¿å­˜é‡‡é›†æ•°æ®åˆ°æ•°æ®åº“

        Args:
            data_items: æ•°æ®é¡¹åˆ—è¡¨

        Returns:
            ä¿å­˜çš„æ•°é‡
        """
        saved_count = 0
        
        for item in data_items:
            try:
                collection_data = CollectionData(
                    title=item.get('title', ''),
                    url=item.get('url', ''),
                    summary=item.get('summary', ''),
                    source=item.get('source', ''),
                    image_url=item.get('image_url', ''),
                    keyword=item.get('keyword', ''),
                    source_type=item.get('source_type', ''),
                    raw_data=item.get('raw_data', ''),
                    collected_at=datetime.utcnow()
                )
                db.session.add(collection_data)
                saved_count += 1
            except Exception as e:
                print(f"Error saving data: {e}")
        
        if saved_count > 0:
            db.session.commit()
        
        return saved_count

    def get_saved_data(self, keyword=None, source_type=None, limit=50, offset=0):
        """
        è·å–å·²ä¿å­˜çš„æ•°æ®

        Args:
            keyword: å…³é”®è¯è¿‡æ»¤
            source_type: æºç±»å‹è¿‡æ»¤
            limit: é™åˆ¶æ•°é‡
            offset: åç§»é‡

        Returns:
            æ•°æ®åˆ—è¡¨
        """
        query = CollectionData.query
        
        if keyword:
            query = query.filter(CollectionData.keyword.contains(keyword))
        if source_type:
            query = query.filter_by(source_type=source_type)
        
        query = query.order_by(CollectionData.saved_at.desc())
        
        if limit:
            query = query.limit(limit)
        if offset:
            query = query.offset(offset)
        
        return query.all()

    def delete_saved_data(self, data_id):
        """
        åˆ é™¤å·²ä¿å­˜çš„æ•°æ®

        Args:
            data_id: æ•°æ®ID

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        data = CollectionData.query.get(data_id)
        if data:
            db.session.delete(data)
            db.session.commit()
            return True
        return False

    def stop_collection(self, collection_id):
        """
        åœæ­¢é‡‡é›†ä»»åŠ¡

        Args:
            collection_id: é‡‡é›†ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if collection_id in self.active_collections:
            self.active_collections[collection_id]['status'] = 'stopped'
            return True
        return False

    def get_collection_status(self, collection_id):
        """
        è·å–é‡‡é›†ä»»åŠ¡çŠ¶æ€

        Args:
            collection_id: é‡‡é›†ä»»åŠ¡ID

        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        if collection_id not in self.active_collections:
            return {'status': 'not_found'}
        
        return {
            'status': self.active_collections[collection_id]['status'],
            'keyword': self.active_collections[collection_id]['keyword'],
            'started_at': self.active_collections[collection_id]['started_at'].strftime('%Y-%m-%d %H:%M:%S')
        }


collection_service = CollectionService()
