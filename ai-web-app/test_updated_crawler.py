from app import create_app, db
from app.models import CrawlerSource
from app.services.universal_crawler import create_crawler

def test_updated_crawler():
    """æµ‹è¯•æ›´æ–°åçš„çˆ¬è™«"""
    app = create_app()
    
    with app.app_context():
        baidu_source = CrawlerSource.query.filter_by(name='ç™¾åº¦æœç´¢').first()
        
        if not baidu_source:
            print("âŒ æœªæ‰¾åˆ°ç™¾åº¦æœç´¢çˆ¬è™«æº")
            return
        
        print(f"âœ… æ‰¾åˆ°ç™¾åº¦æœç´¢çˆ¬è™«æº (ID: {baidu_source.id})")
        print(f"   åç§°: {baidu_source.name}")
        print(f"   ç±»å‹: {baidu_source.source_type}")
        
        source_config = {
            'name': baidu_source.name,
            'source_type': baidu_source.source_type,
            'url': baidu_source.url,
            'method': baidu_source.method,
            'headers': baidu_source.headers,
            'body_template': baidu_source.body_template,
            'data_selector': baidu_source.data_selector,
            'title_selector': baidu_source.title_selector,
            'url_selector': baidu_source.url_selector,
            'summary_selector': baidu_source.summary_selector,
            'image_selector': baidu_source.image_selector,
            'config': baidu_source.config
        }
        
        print("\nğŸ” å¼€å§‹æµ‹è¯•çˆ¬è™«...")
        print("=" * 60)
        
        try:
            crawler = create_crawler(source_config)
            print(f"âœ… æˆåŠŸåˆ›å»ºçˆ¬è™«å®ä¾‹: {type(crawler).__name__}")
            
            results = crawler.crawl('æˆéƒ½', 1, 10)
            
            print(f"âœ… çˆ¬å–æˆåŠŸï¼å…±è·å– {len(results)} æ¡ç»“æœ")
            print("=" * 60)
            
            for i, result in enumerate(results[:5], 1):
                print(f"\nç»“æœ {i}:")
                print(f"  æ ‡é¢˜: {result.get('title', 'N/A')}")
                print(f"  URL: {result.get('url', 'N/A')}")
                print(f"  æ‘˜è¦: {result.get('summary', 'N/A')[:100]}...")
            
            if len(results) > 5:
                print(f"\n... è¿˜æœ‰ {len(results) - 5} æ¡ç»“æœ")
            
            crawler.close()
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

if __name__ == '__main__':
    test_updated_crawler()
